[{"authors":["admin"],"categories":null,"content":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My current research work mostly focuses on modern instances of Experimental Design \u0026ndash; a branch of statistics addressing the search for the most informative experiments in order to infer an unknown statistical quantity.\nI am also interested in/have worked on Bayesian optimization, kernel methods, sensor selection, control, bandit algorithms and convex optimization. Some of my algorithms have found application in machine calibration, spatial analysis and directed evolution.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mojmirmutny.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My current research work mostly focuses on modern instances of Experimental Design \u0026ndash; a branch of statistics addressing the search for the most informative experiments in order to infer an unknown statistical quantity.","tags":null,"title":"Mojmír Mutný","type":"authors"},{"authors":["Elvis Nava","Mojmír Mutný","Andreas Krause"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956760,"objectID":"b883372cc7625abc21d14c9e4e13c981","permalink":"https://mojmirmutny.github.io/publication/nava-2022/","publishdate":"2022-02-27T10:12:40.75186Z","relpermalink":"/publication/nava-2022/","section":"publication","summary":"In this work we introduced DPP-BBO, a natural and easily applicable framework for enhancing batch diversity in BBO algorithms which works in more settings than previous diversification strategies: it is directly applicable to the continuous domain case, when due to approximation and non-standard models we are unable to compute hallucinations or confidence intervals (as in the Cox process example), or more generally when used in combination with any randomized BBO sampling scheme or arbitrary diversity kernel. Moreover, for DPP-TS we show improved theoretical guarantees and strong practical performance on simple regret.","tags":[],"title":"Diversified Sampling for Batched Bayesian Optimization with Determinantal Point Processes","type":"publication"},{"authors":["Mojm\\ŕ Mutný","Andreas Krause"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956761,"objectID":"3d4d24b3e8159b7ee5d7a9b375a2c069","permalink":"https://mojmirmutny.github.io/publication/mutny-2022-b/","publishdate":"2022-02-27T10:12:41.047168Z","relpermalink":"/publication/mutny-2022-b/","section":"publication","summary":"Optimal experimental design seeks to determine the most informative allocation of experiments  to infer an unknown statistical quantity. In this work, we investigate optimal design of experiments for em estimation of linear functionals in reproducing kernel Hilbert spaces (RKHSs). This problem has been extensively studied in the linear regression setting under an estimability condition, which allows estimating parameters without bias. We generalize this framework to RKHSs, and allow for the linear functional to be only approximately inferred, i.e., with a fixed bias. This scenario captures many important modern applications such as estimation of gradient maps, integrals and solutions to differential equations. We provide algorithms for constructing bias-aware designs for linear functionals. We derive non-asymptotic confidence sets for fixed and adaptive designs under sub-Gaussian noise, enabling us to certify estimation with bounded error with high probability.","tags":[],"title":"Experimental Design of Linear Functionals in Reproducing Kernel Hilbert Spaces","type":"publication"},{"authors":["Zalán Borsos","Mojmír Mutný","Marco Tagliasacchi","Andreas Krause"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956761,"objectID":"6ea660e2c6fd8b51773ef9aa21db1bf4","permalink":"https://mojmirmutny.github.io/publication/borsos-2021-data/","publishdate":"2022-02-27T10:12:41.630116Z","relpermalink":"/publication/borsos-2021-data/","section":"publication","summary":"The increasing availability of massive data sets poses a series of challenges for machine learning. Prominent among these is the need to learn models under hardware or human resource constraints. In such resource-constrained settings, a simple yet powerful approach is to operate on small subsets of the data. Coresets are weighted subsets of the data that provide approximation guarantees for the optimization objective. However, existing coreset constructions are highly model-specific and are limited to simple models such as linear regression, logistic regression, and k-means. In this work, we propose a generic coreset construction framework that formulates the coreset selection as a cardinality-constrained bilevel optimization problem. In contrast to existing approaches, our framework does not require model-specific adaptations and applies to any twice differentiable model, including neural networks. We show the effectiveness of our framework for a wide range of models in various settings, including training non-convex models online and batch active learning.","tags":[],"title":"Data Summarization via Bilevel Optimization","type":"publication"},{"authors":["Marc Jourdan","Mojm\\ŕ Mutný","Johannes Kirschner","Andreas Krause"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956761,"objectID":"f80af68aa130413142a9f2317e28d5fb","permalink":"https://mojmirmutny.github.io/publication/jourdan-2021/","publishdate":"2022-02-27T10:12:41.349816Z","relpermalink":"/publication/jourdan-2021/","section":"publication","summary":"Combinatorial bandits with semi-bandit feedback generalize multi-armed bandits, where the agent chooses sets of arms and observes a noisy reward for each arm contained in the chosen set. The action set satisfies a given structure such as forming a base of a matroid or a path in a graph. We focus on the pure-exploration problem of identifying the best arm with fixed confidence, as well as a more general setting, where the structure of the answer set differs from the one of the action set. Using the recently popularized game framework, we interpret this problem as a sequential zero-sum game and develop a CombGame meta-algorithm whose instances are asymptotically optimal algorithms with finite time guarantees. In addition to comparing two families of learners to instantiate our meta-algorithm, the main contribution of our work is a specific oracle efficient instance for best-arm identification with combinatorial actions. Based on a projection-free online learning algorithm for convex polytopes, it is the first computationally efficient algorithm which is asymptotically optimal and has competitive empirical performance.","tags":[],"title":"Efficient Pure Exploration for Combinatorial Bandits with Semi-Bandit Feedback","type":"publication"},{"authors":["Lenart Treven","Sebastian Curi","Mojmír Mutn\\ ́and Andreas Krause"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956760,"objectID":"db99b1408aa9e8a0fa46d7b2cce7f814","permalink":"https://mojmirmutny.github.io/publication/treven-2021/","publishdate":"2022-02-27T10:12:39.763865Z","relpermalink":"/publication/treven-2021/","section":"publication","summary":"We present the first approach for learning--from a single trajectory--a linear quadratic regulator (LQR), even for unstable systems, without knowledge of the system dynamics and without requiring an initial stabilizing controller. Our central contribution is an efficient algorithm--emph eXploration--that quickly identifies a stabilizing controller. Our approach utilizes robust System Level Synthesis (SLS), and we prove that it succeeds in a constant number of iterations. Our approach can be used to initialize existing algorithms that require a stabilizing controller as input. When used in this way, it yields a method for learning LQRs from a single trajectory and even for unstable systems, while suffering at most  regret.","tags":[],"title":"Learning Controllers for Unstable Linear Quadratic Regulators from a Single Trajectory","type":"publication"},{"authors":["Mojmír Mutný","Andreas Krause"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956760,"objectID":"8b7e3e1fed47d34f8005baf983d040b9","permalink":"https://mojmirmutny.github.io/publication/mutny-2021-a/","publishdate":"2022-02-27T10:12:40.065165Z","relpermalink":"/publication/mutny-2021-a/","section":"publication","summary":"Inhomogeneous Poisson point processes are widely used  models of event occurrences. We address emphadaptive sensing of Poisson Point processes, namely, maximizing the number of captured events subject to sensing costs. We encode prior assumptions on the rate function by modeling it as a member of a known emphreproducing kernel Hilbert space (RKHS). By partitioning the domain into separate small regions, and using heteroscedastic linear regression, we propose a tractable estimator of Poisson process rates for two feedback models: emphcount-record, where exact locations of events are observed, and emphhistogram feedback, where only counts of events are observed. We derive provably accurate anytime confidence estimates for our estimators for sequentially acquired Poisson count data. Using these, we formulate algorithms based on optimism that provably incur sublinear count-regret. We demonstrate the practicality of the method on problems from crime modeling, revenue maximization as well as environmental monitoring.","tags":[],"title":"No-regret Algorithms for Capturing Events in Poisson Point Processes","type":"publication"},{"authors":["Javier Tapia","Espen Knoop","Mojmir Mutnỳ","Miguel A Otaduy","Moritz Bächer"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956759,"objectID":"1c8aec2bdc4e519bfa419c51c79cc8da","permalink":"https://mojmirmutny.github.io/publication/tapia-2020-makesense/","publishdate":"2022-02-27T10:12:39.023888Z","relpermalink":"/publication/tapia-2020-makesense/","section":"publication","summary":"Soft robots have applications in safe human–robot interactions, manipulation of fragile objects, and locomotion in challenging and unstructured environments. In this article, we present a computational method for augmenting soft robots with proprioceptive sensing capabilities. Our method automatically computes a minimal stretch-receptive sensor network to user-provided soft robotic designs, which is optimized to perform well under a set of user-specified deformation-force pairs. The sensorized robots are able to reconstruct their full deformation state, under interaction forces. We cast our sensor design as a subselection problem, selecting a minimal set of sensors from a large set of fabricable ones, which minimizes the error when sensing specified deformation-force pairs. Unique to our approach is the use of an analytical gradient of our reconstruction performance measure with respect to selection variables. We demonstrate our technique on a bending bar and gripper example, illustrating more complex designs with a simulated tentacle.","tags":[],"title":"MakeSense: Automated Sensor Design for Proprioceptive Soft Robots","type":"publication"},{"authors":["Mojmir Mutný","Michal Derezisnki","Andreas Krause"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956758,"objectID":"1efd87d84f64a8765b8f519769daf248","permalink":"https://mojmirmutny.github.io/publication/mutny-2020-b/","publishdate":"2022-02-27T10:12:38.634733Z","relpermalink":"/publication/mutny-2020-b/","section":"publication","summary":"We analyze the convergence rate of the randomized Newton-like method introduced by Qu et. al. (2016) for smooth and convex objectives, which uses random coordinate blocks of a Hessian-over-approximation matrix $퐌$ instead of the true Hessian. The convergence analysis of the algorithm is challenging because of its complex dependence on the structure of $퐌$. However, we show that when the coordinate blocks are sampled with probability proportional to their determinant, the convergence rate depends solely on the eigenvalue distribution of matrix $퐌$, and has an analytically tractable form. To do so, we derive a fundamental new expectation formula for determinantal point processes. We show that determinantal sampling allows us to reason about the optimal subset size of blocks in terms of the spectrum of $퐌$. Additionally, we provide a numerical evaluation of our analysis, demonstrating cases where determinantal sampling is superior or on par with uniform sampling.","tags":[],"title":"Convergence Analysis of Block Coordinate Algorithms with Determinantal Sampling","type":"publication"},{"authors":["Zalán Borsos","Mojm\\ŕ Mutn\\'ánd Andreas Krause"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956759,"objectID":"fa0c7e675bcfe0789b50866a7b821c6e","permalink":"https://mojmirmutny.github.io/publication/borsos-2020/","publishdate":"2022-02-27T10:12:39.464096Z","relpermalink":"/publication/borsos-2020/","section":"publication","summary":"Coresets are small data summaries that are sufficient for model training. They can be maintained online, enabling efficient handling of large data streams under resource constraints. However, existing constructions are limited to simple models such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efficiently generate coresets for deep neural networks, and demonstrate its empirical benefits in continual learning and in streaming settings.","tags":[],"title":"Coresets via Bilevel Optimization for Continual Learning and Streaming","type":"publication"},{"authors":["Mojmir Mutný","Johannes Kirschner","Andreas Krause"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956758,"objectID":"f83907aca901d104ed36435314c941e7","permalink":"https://mojmirmutny.github.io/publication/mutny-2020/","publishdate":"2022-02-27T10:12:38.337113Z","relpermalink":"/publication/mutny-2020/","section":"publication","summary":"Bayesian optimization and kernelized bandit algorithms are widely used techniques for sequential black box function optimization with applications in parameter tuning, control, robotics among many others. To be effective in high dimensional settings, previous approaches make additional assumptions, for example on low-dimensional subspaces or an additive structure. In this work, we go beyond the additivity assumption and use an orthogonal projection pursuit regression model, which strictly generalizes additive models. We present a two-stage algorithm motivated by experimental design to first decorrelate the additive components. Subsequently, the bandit optimization benefits from the statistically efficient additive model. Our method provably decorrelates the fully additive model and achieves optimal sublinear simple regret in terms of the number of function evaluations. To prove the rotation recovery, we derive novel concentration inequalities for linear regression on subspaces. In addition, we specifically address the issue of acquisition function optimization and present two domain dependent efficient algorithms. We validate the algorithm numerically on synthetic as well as real-world optimization problems.","tags":[],"title":"Experimental Design for Optimization of Orthogonal Projection Pursuit Models","type":"publication"},{"authors":["Mojmír Mutný","Andreas Krause"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956760,"objectID":"9c072c5e6f52d3200d845dab14571b34","permalink":"https://mojmirmutny.github.io/publication/mutny-2022/","publishdate":"2022-02-27T10:12:40.534205Z","relpermalink":"/publication/mutny-2022/","section":"publication","summary":"We study adaptive sensing of Cox point processes, a widely used model from spatial statistics. We introduce three tasks: maximization of captured events, search for the maximum of the intensity function and learning level sets of the intensity function. We model the intensity function as a sample from a truncated Gaussian process, represented in a specially constructed positive basis. In this basis, the positivity constraint on the intensity function has a simple form. We show how the emphminimal description positive basis can be adapted to the covariance kernel, to non-stationarity and make connections to common positive bases from prior works. Our adaptive sensing algorithms use Langevin dynamics and are based on posterior sampling (textscCox-Thompson) and top-two posterior sampling (textscTop2) principles. With latter, the difference between samples serves as a surrogate to the uncertainty. We demonstrate the approach using examples from environmental monitoring and crime rate modeling, and compare it to the classical Bayesian experimental design approach.","tags":[],"title":"Sensing Cox Processes via Posterior Sampling and Positive Bases","type":"publication"},{"authors":["Johannes Kirschner","Mojmir Mutný","Nicole Hiller","Rasmus Ischebeck","Andreas Krause"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956760,"objectID":"4e18e812d151f86eb355a9a93f0e3fb8","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-linebo/","publishdate":"2022-02-27T10:12:40.267364Z","relpermalink":"/publication/kirschner-2019-linebo/","section":"publication","summary":"Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.","tags":[],"title":"Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces","type":"publication"},{"authors":["Johannes Kirschner","Manuel Nonnenmacher","Mojmir Mutný","Nicole Hiller","Andreas Adelmann","Rasmus Ischebeck","Andreas Krause"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956758,"objectID":"96b029387f09ac2059d72b0b85d1ab85","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-swissfel/","publishdate":"2022-02-27T10:12:38.841578Z","relpermalink":"/publication/kirschner-2019-swissfel/","section":"publication","summary":"Parameter tuning is a notoriously time-consuming task in accelerator facilities. As tool for global optimization with noisy evaluations, Bayesian optimization was recently shown to outperform alternative methods. By learning a model of the underlying function using all available data, the next evaluation can be chosen carefully to find the optimum with as few steps as possible and without violating any safety constraints. However, the per-step computation time increases significantly with the number of parameters and the generality of the approach can lead to slow convergence on functions that are easier to optimize. To overcome these limitations, we divide the global problem into sequential subproblems that can be solved efficiently using safe Bayesian optimization. This allows us to trade off local and global convergence and to adapt to additional structure in the objective function. Further, we provide slice-plots of the function as user feedback during the optimization. We showcase how we use our algorithm to tune up the FEL output of SwissFEL with up to 40 parameters simultaneously, and reach convergence within reasonable tuning times in the order of 30 minutes (","tags":[],"title":"Bayesian Optimization for Fast and Safe Parameter Tuning of SwissFEL","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"6bbfdc3a3d5d292dc9c0b936535947a6","permalink":"https://mojmirmutny.github.io/code/qff/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/code/qff/","section":"code","summary":"This repository includes the code used in paper:Mojmir Mutny \u0026 Andreas Krause, \"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features\", NIPS 2018\nIt provides an efficient finite basis approximation for RBF and Matern kernels in low dimensions.","tags":null,"title":"Quadrature Fourier Features (QFF) for Python","type":"code"},{"authors":["Mojmir Mutný","Andreas Krause"],"categories":[],"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956758,"objectID":"d5f311b04b2beb842fcf6726e943a8a7","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-b/","publishdate":"2022-02-27T10:12:37.908164Z","relpermalink":"/publication/mutny-2018-b/","section":"publication","summary":"We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regretemph polynomial time(in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreasesemph exponentially with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.","tags":[],"title":"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features","type":"publication"},{"authors":["Mojmı́r Mutný","Peter Richtárik"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956758,"objectID":"fd7fb20a1c9a87626941ed28d1014427","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-a/","publishdate":"2022-02-27T10:12:38.054866Z","relpermalink":"/publication/mutny-2018-a/","section":"publication","summary":"We propose a parallel stochastic Newton method (PSN) for minimizing unconstrained smooth convex functions. We analyze the method in the strongly convex case, and give conditions under which acceleration can be expected when compared to its serial counter-part. We show how PSN can be applied to the large quadratic function minimization in general, and empirical risk minimization problems. We demonstrate the practical efficiency of the method through numerical experiments and models of simple matrix classes.","tags":[],"title":"Parallel Stochastic Newton Method","type":"publication"},{"authors":["Mojmir Mutný"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645956759,"objectID":"3413189ff029343f3f81b969d0ca8477","permalink":"https://mojmirmutny.github.io/publication/mutny-2016-stochastic/","publishdate":"2022-02-27T10:12:39.210673Z","relpermalink":"/publication/mutny-2016-stochastic/","section":"publication","summary":"A stochastic iterative algorithm approximating second-order information using von Neumann series is discussed. We present convergence guarantees for strongly-convex and smooth functions. Our analysis is much simpler in contrast to a similar algorithm and its analysis, LISSA. The algorithm is primarily suitable for training large scale linear models, where the number of data points is very large. Two novel analyses, one showing space independent linear convergence, and one showing conditional quadratic convergence are discussed. In numerical experiments, the behavior of the error is similar to the second-order algorithm L-BFGS, and improves the performance of LISSA for quadratic objective function.","tags":[],"title":"Stochastic second-order optimization via von Neumann series","type":"publication"}]