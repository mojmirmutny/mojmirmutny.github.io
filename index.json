[{"authors":["admin"],"categories":null,"content":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My research focuses on optimization methods for machine learning, and machine learning methods used as optimization tools among other things. I work mostly on Experimental Design, Active Learning and Bandit Algorithms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mojmirmutny.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My research focuses on optimization methods for machine learning, and machine learning methods used as optimization tools among other things.","tags":null,"title":"Mojmír Mutný","type":"authors"},{"authors":["Mojmir Mutný","Michal Derezisnki","Andreas Krause"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"1efd87d84f64a8765b8f519769daf248","permalink":"https://mojmirmutny.github.io/publication/mutny-2020-b/","publishdate":"2020-08-19T13:21:48.589565Z","relpermalink":"/publication/mutny-2020-b/","section":"publication","summary":"We analyze the convergence rate of the randomized Newton-like method introduced by Qu et. al. (2016) for smooth and convex objectives, which uses random coordinate blocks of a Hessian-over-approximation matrix $bM$ instead of the true Hessian. The convergence analysis of the algorithm is challenging because of its complex dependence on the structure of $bM$. However, we show that when the coordinate blocks are sampled with probability proportional to their determinant, the convergence rate depends solely on the eigenvalue distribution of matrix $bM$, and has an analytically tractable form. To do so, we derive a fundamental new expectation formula for determinantal point processes. We show that determinantal sampling allows us to reason about the optimal subset size of blocks in terms of the spectrum of $bM$. Additionally, we provide a numerical evaluation of our analysis, demonstrating cases where determinantal sampling is superior or on par with uniform sampling.","tags":null,"title":"Convergence Analysis of Block Coordinate Algorithms with Determinantal Sampling","type":"publication"},{"authors":["Mojmir Mutný","Johannes Kirschner","Andreas Krause"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"f83907aca901d104ed36435314c941e7","permalink":"https://mojmirmutny.github.io/publication/mutny-2020/","publishdate":"2020-08-19T13:21:48.589279Z","relpermalink":"/publication/mutny-2020/","section":"publication","summary":"Bayesian optimization and kernelized bandit algorithms are widely used techniques for sequential black box function optimization with applications in parameter tuning, control, robotics among many others. To be effective in high dimensional settings, previous approaches make additional assumptions, for example on low-dimensional subspaces or an additive structure. In this work, we go beyond the additivity assumption and use an orthogonal projection pursuit regression model, which strictly generalizes additive models. We present a two-stage algorithm motivated by experimental design to first decorrelate the additive components. Subsequently, the bandit optimization benefits from the statistically efficient additive model. Our method provably decorrelates the fully additive model and achieves optimal sublinear simple regret in terms of the number of function evaluations. To prove the rotation recovery, we derive novel concentration inequalities for linear regression on subspaces. In addition, we specifically address the issue of acquisition function optimization and present two domain dependent efficient algorithms. We validate the algorithm numerically on synthetic as well as real-world optimization problems.","tags":null,"title":"Experimental Design for Optimization of Orthogonal Projection Pursuit Models","type":"publication"},{"authors":["Zalan Borsos","Mojmir Mutny","Andreas Krause"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fa0c7e675bcfe0789b50866a7b821c6e","permalink":"https://mojmirmutny.github.io/publication/borsos-2020/","publishdate":"2020-08-19T13:21:48.59095Z","relpermalink":"/publication/borsos-2020/","section":"publication","summary":"","tags":null,"title":"Coresets via Bilevel Optimization for Continual Learning and Streaming","type":"publication"},{"authors":["L Treven","S Curi","M Mutny","A Krause"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"6f753b82434c3702de1abb9b45007689","permalink":"https://mojmirmutny.github.io/publication/treven-2020/","publishdate":"2020-08-19T13:21:48.591147Z","relpermalink":"/publication/treven-2020/","section":"publication","summary":"","tags":null,"title":"Learning Controllers for Unstable Linear Quadratic Regulators from a Single Trajectory","type":"publication"},{"authors":["Johannes Kirschner","Mojmir Mutný","Nicole Hiller","Rasmus Ischebeck","Andreas Krause"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"4e18e812d151f86eb355a9a93f0e3fb8","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-linebo/","publishdate":"2020-08-19T13:21:48.589859Z","relpermalink":"/publication/kirschner-2019-linebo/","section":"publication","summary":"Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. Our method scales well to high dimensions and makes use of a global Gaussian process model. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.","tags":null,"title":"Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces","type":"publication"},{"authors":["Johannes Kirschner","Manuel Nonnenmacher","Mojmir Mutný","Nicole Hiller","Andreas Adelmann","Rasmus Ischebeck","Andreas Krause"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"96b029387f09ac2059d72b0b85d1ab85","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-swissfel/","publishdate":"2020-08-19T13:21:48.59016Z","relpermalink":"/publication/kirschner-2019-swissfel/","section":"publication","summary":"Parameter tuning is a notoriously time-consuming task in accelerator facilities. As tool for global optimization with noisy evaluations, Bayesian optimization was recently shown to outperform alternative methods. By learning a model of the underlying function using all available data, the next evaluation can be chosen carefully to find the optimum with as few steps as possible and without violating any safety constraints. However, the per-step computation time increases significantly with the number of parameters and the generality of the approach can lead to slow convergence on functions that are easier to optimize. To overcome these limitations, we divide the global problem into sequential subproblems that can be solved efficiently using safe Bayesian optimization. This allows us to trade off local and global convergence and to adapt to additional structure in the objective function. Further, we provide slice-plots of the function as user feedback during the optimization. We showcase how we use our algorithm to tune up the FEL output of SwissFEL with up to 40 parameters simultaneously, and reach convergence within reasonable tuning times in the order of 30 minutes (","tags":null,"title":"Bayesian Optimization for Fast and Safe Parameter Tuning of SwissFEL","type":"publication"},{"authors":["Javier Tapia","Espen Knoop","Mojmir Mutnỳ","Miguel A Otaduy","Moritz Bächer"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c6c10493fdce7d4f934d95da869470cd","permalink":"https://mojmirmutny.github.io/publication/tapia-2019-makesense/","publishdate":"2020-08-19T13:21:48.590453Z","relpermalink":"/publication/tapia-2019-makesense/","section":"publication","summary":"Soft robots have applications in safe human–robot interactions, manipulation of fragile objects, and locomotion in challenging and unstructured environments. In this article, we present a computational method for augmenting soft robots with proprioceptive sensing capabilities. Our method automatically computes a minimal stretch-receptive sensor network to user-provided soft robotic designs, which is optimized to perform well under a set of user-specified deformation-force pairs. The sensorized robots are able to reconstruct their full deformation state, under interaction forces. We cast our sensor design as a subselection problem, selecting a minimal set of sensors from a large set of fabricable ones, which minimizes the error when sensing specified deformation-force pairs. Unique to our approach is the use of an analytical gradient of our reconstruction performance measure with respect to selection variables. We demonstrate our technique on a bending bar and gripper example, illustrating more complex designs with a simulated tentacle.","tags":null,"title":"MakeSense: Automated Sensor Design for Proprioceptive Soft Robots","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"6bbfdc3a3d5d292dc9c0b936535947a6","permalink":"https://mojmirmutny.github.io/code/qff/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/code/qff/","section":"code","summary":"This repository includes the code used in paper:Mojmir Mutny \u0026 Andreas Krause, \"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features\", NIPS 2018\nIt provides an efficient finite basis approximation for RBF and Matern kernels in low dimensions.","tags":null,"title":"Quadrature Fourier Features (QFF) for Python","type":"code"},{"authors":["Mojmir Mutný","Andreas Krause"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"d5f311b04b2beb842fcf6726e943a8a7","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-b/","publishdate":"2020-08-19T13:21:48.588544Z","relpermalink":"/publication/mutny-2018-b/","section":"publication","summary":"We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regretemph polynomial time(in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreasesemph exponentially with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.","tags":null,"title":"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features","type":"publication"},{"authors":["Mojmı́r Mutný","Peter Richtárik"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"fd7fb20a1c9a87626941ed28d1014427","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-a/","publishdate":"2020-08-19T13:21:48.588998Z","relpermalink":"/publication/mutny-2018-a/","section":"publication","summary":"We propose a parallel stochastic Newton method (PSN) for minimizing unconstrained smooth convex functions. We analyze the method in the strongly convex case, and give  conditions under which acceleration can be expected when compared to its serial counter- part. We show how PSN can be applied to the large quadratic function minimization in  general, and empirical risk minimization problems. We demonstrate the practical efficiency of the method through numerical experiments and models of simple matrix classes.","tags":null,"title":"Parallel Stochastic Newton Method","type":"publication"},{"authors":["Mojmir Mutný"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"3413189ff029343f3f81b969d0ca8477","permalink":"https://mojmirmutny.github.io/publication/mutny-2016-stochastic/","publishdate":"2020-08-19T13:21:48.590715Z","relpermalink":"/publication/mutny-2016-stochastic/","section":"publication","summary":"A stochastic iterative algorithm approximating second-order information using von Neumann series is discussed. We present convergence guarantees for strongly-convex and smooth functions. Our analysis is much simpler in contrast to a similar algorithm and its analysis, LISSA. The algorithm is primarily suitable for training large scale linear models, where the number of data points is very large. Two novel analyses, one showing space independent linear convergence, and one showing conditional quadratic convergence are discussed. In numerical experiments, the behavior of the error is similar to the second-order algorithm L-BFGS, and improves the performance of LISSA for quadratic objective function.","tags":null,"title":"Stochastic second-order optimization via von Neumann series","type":"publication"}]