[{"authors":["admin"],"categories":null,"content":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My research focuses on optimization methods for machine learning, and machine learning methods used as optimization tools among other things. I work mostly on Experimental Design, Active Learning and Bandit Algorithms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mojmirmutny.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student at ETH Zurich under the supervisior of Andreas Krause in Learning and Adaptive Systems group. My research focuses on optimization methods for machine learning, and machine learning methods used as optimization tools among other things.","tags":null,"title":"Mojmír Mutný","type":"authors"},{"authors":["Mojmír Mutn\\ ́and Andreas Krause"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626252893,"objectID":"8b7e3e1fed47d34f8005baf983d040b9","permalink":"https://mojmirmutny.github.io/publication/mutny-2021-a/","publishdate":"2021-07-14T08:54:53.694029Z","relpermalink":"/publication/mutny-2021-a/","section":"publication","summary":"Inhomogeneous Poisson point processes are widely used  models of event occurrences. We address emphadaptive sensing of Poisson Point processes, namely, maximizing the number of captured events subject to sensing costs. We encode prior assumptions on the rate function by modeling it as a member of a known emphreproducing kernel Hilbert space (RKHS). By partitioning the domain into separate small regions, and using heteroscedastic linear regression, we propose a tractable estimator of Poisson process rates for two feedback models: emphcount-record, where exact locations of events are observed, and emphhistogram feedback, where only counts of events are observed. We derive provably accurate anytime confidence estimates for our estimators for sequentially acquired Poisson count data. Using these, we formulate algorithms based on optimism that provably incur sublinear count-regret. We demonstrate the practicality of the method on problems from crime modeling, revenue maximization as well as environmental monitoring.","tags":[],"title":"No-regret Algorithms for Capturing Events in Poisson Point Processes","type":"publication"},{"authors":["Lenart Treven","Sebastian Curi","Mojmir Mutny","Andreas Krause"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f753b82434c3702de1abb9b45007689","permalink":"https://mojmirmutny.github.io/publication/treven-2020/","publishdate":"2021-07-14T08:54:53.605155Z","relpermalink":"/publication/treven-2020/","section":"publication","summary":"We present the first approach for learning--from a single trajectory--a linear quadratic regulator (LQR), even for unstable systems, without knowledge of the system dynamics and without requiring an initial stabilizing controller. Our central contribution is an efficient algorithm--emph eXploration--that quickly identifies a stabilizing controller. Our approach utilizes robust System Level Synthesis (SLS), and we prove that it succeeds in a constant number of iterations. Our approach can be used to initialize existing algorithms that require a stabilizing controller as input. When used in this way, it yields a method for learning LQRs from a single trajectory and even for unstable systems, while suffering at most  regret.","tags":null,"title":"Learning Controllers for Unstable Linear Quadratic Regulators from a Single Trajectory","type":"publication"},{"authors":["Mojmir Mutný","Michal Derezisnki","Andreas Krause"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"1efd87d84f64a8765b8f519769daf248","permalink":"https://mojmirmutny.github.io/publication/mutny-2020-b/","publishdate":"2021-07-14T08:54:53.167297Z","relpermalink":"/publication/mutny-2020-b/","section":"publication","summary":"We analyze the convergence rate of the randomized Newton-like method introduced by Qu et. al. (2016) for smooth and convex objectives, which uses random coordinate blocks of a Hessian-over-approximation matrix $bM$ instead of the true Hessian. The convergence analysis of the algorithm is challenging because of its complex dependence on the structure of $bM$. However, we show that when the coordinate blocks are sampled with probability proportional to their determinant, the convergence rate depends solely on the eigenvalue distribution of matrix $bM$, and has an analytically tractable form. To do so, we derive a fundamental new expectation formula for determinantal point processes. We show that determinantal sampling allows us to reason about the optimal subset size of blocks in terms of the spectrum of $bM$. Additionally, we provide a numerical evaluation of our analysis, demonstrating cases where determinantal sampling is superior or on par with uniform sampling.","tags":null,"title":"Convergence Analysis of Block Coordinate Algorithms with Determinantal Sampling","type":"publication"},{"authors":["Mojmir Mutný","Johannes Kirschner","Andreas Krause"],"categories":null,"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"f83907aca901d104ed36435314c941e7","permalink":"https://mojmirmutny.github.io/publication/mutny-2020/","publishdate":"2021-07-14T08:54:53.078691Z","relpermalink":"/publication/mutny-2020/","section":"publication","summary":"Bayesian optimization and kernelized bandit algorithms are widely used techniques for sequential black box function optimization with applications in parameter tuning, control, robotics among many others. To be effective in high dimensional settings, previous approaches make additional assumptions, for example on low-dimensional subspaces or an additive structure. In this work, we go beyond the additivity assumption and use an orthogonal projection pursuit regression model, which strictly generalizes additive models. We present a two-stage algorithm motivated by experimental design to first decorrelate the additive components. Subsequently, the bandit optimization benefits from the statistically efficient additive model. Our method provably decorrelates the fully additive model and achieves optimal sublinear simple regret in terms of the number of function evaluations. To prove the rotation recovery, we derive novel concentration inequalities for linear regression on subspaces. In addition, we specifically address the issue of acquisition function optimization and present two domain dependent efficient algorithms. We validate the algorithm numerically on synthetic as well as real-world optimization problems.","tags":null,"title":"Experimental Design for Optimization of Orthogonal Projection Pursuit Models","type":"publication"},{"authors":["Zalán Borsos","Mojm\\ŕ Mutn\\'ánd Andreas Krause"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fa0c7e675bcfe0789b50866a7b821c6e","permalink":"https://mojmirmutny.github.io/publication/borsos-2020/","publishdate":"2021-07-14T08:54:53.519988Z","relpermalink":"/publication/borsos-2020/","section":"publication","summary":"Coresets are small data summaries that are sufficient for model training. They can be maintained online, enabling efficient handling of large data streams under resource constraints. However, existing constructions are limited to simple models such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efficiently generate coresets for deep neural networks, and demonstrate its empirical benefits in continual learning and in streaming settings.","tags":null,"title":"Coresets via Bilevel Optimization for Continual Learning and Streaming","type":"publication"},{"authors":["Johannes Kirschner","Mojmir Mutný","Nicole Hiller","Rasmus Ischebeck","Andreas Krause"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"4e18e812d151f86eb355a9a93f0e3fb8","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-linebo/","publishdate":"2021-07-14T08:54:53.790507Z","relpermalink":"/publication/kirschner-2019-linebo/","section":"publication","summary":"","tags":null,"title":"Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces","type":"publication"},{"authors":["Johannes Kirschner","Manuel Nonnenmacher","Mojmir Mutný","Nicole Hiller","Andreas Adelmann","Rasmus Ischebeck","Andreas Krause"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"96b029387f09ac2059d72b0b85d1ab85","permalink":"https://mojmirmutny.github.io/publication/kirschner-2019-swissfel/","publishdate":"2021-07-14T08:54:53.255592Z","relpermalink":"/publication/kirschner-2019-swissfel/","section":"publication","summary":"Parameter tuning is a notoriously time-consuming task in accelerator facilities. As tool for global optimization with noisy evaluations, Bayesian optimization was recently shown to outperform alternative methods. By learning a model of the underlying function using all available data, the next evaluation can be chosen carefully to find the optimum with as few steps as possible and without violating any safety constraints. However, the per-step computation time increases significantly with the number of parameters and the generality of the approach can lead to slow convergence on functions that are easier to optimize. To overcome these limitations, we divide the global problem into sequential subproblems that can be solved efficiently using safe Bayesian optimization. This allows us to trade off local and global convergence and to adapt to additional structure in the objective function. Further, we provide slice-plots of the function as user feedback during the optimization. We showcase how we use our algorithm to tune up the FEL output of SwissFEL with up to 40 parameters simultaneously, and reach convergence within reasonable tuning times in the order of 30 minutes (","tags":null,"title":"Bayesian Optimization for Fast and Safe Parameter Tuning of SwissFEL","type":"publication"},{"authors":["Javier Tapia","Espen Knoop","Mojmir Mutnỳ","Miguel A Otaduy","Moritz Bächer"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c6c10493fdce7d4f934d95da869470cd","permalink":"https://mojmirmutny.github.io/publication/tapia-2019-makesense/","publishdate":"2021-07-14T08:54:53.347677Z","relpermalink":"/publication/tapia-2019-makesense/","section":"publication","summary":"Soft robots have applications in safe human–robot interactions, manipulation of fragile objects, and locomotion in challenging and unstructured environments. In this article, we present a computational method for augmenting soft robots with proprioceptive sensing capabilities. Our method automatically computes a minimal stretch-receptive sensor network to user-provided soft robotic designs, which is optimized to perform well under a set of user-specified deformation-force pairs. The sensorized robots are able to reconstruct their full deformation state, under interaction forces. We cast our sensor design as a subselection problem, selecting a minimal set of sensors from a large set of fabricable ones, which minimizes the error when sensing specified deformation-force pairs. Unique to our approach is the use of an analytical gradient of our reconstruction performance measure with respect to selection variables. We demonstrate our technique on a bending bar and gripper example, illustrating more complex designs with a simulated tentacle.","tags":null,"title":"MakeSense: Automated Sensor Design for Proprioceptive Soft Robots","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546214400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546214400,"objectID":"6bbfdc3a3d5d292dc9c0b936535947a6","permalink":"https://mojmirmutny.github.io/code/qff/","publishdate":"2018-12-31T00:00:00Z","relpermalink":"/code/qff/","section":"code","summary":"This repository includes the code used in paper:Mojmir Mutny \u0026 Andreas Krause, \"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features\", NIPS 2018\nIt provides an efficient finite basis approximation for RBF and Matern kernels in low dimensions.","tags":null,"title":"Quadrature Fourier Features (QFF) for Python","type":"code"},{"authors":["Mojmir Mutný","Andreas Krause"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"d5f311b04b2beb842fcf6726e943a8a7","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-b/","publishdate":"2021-07-14T08:54:52.90251Z","relpermalink":"/publication/mutny-2018-b/","section":"publication","summary":"We develop an efficient and provably no-regret Bayesian optimization (BO) algorithm for optimization of black-box functions in high dimensions. We assume a generalized additive model with possibly overlapping variable groups. When the groups do not overlap, we are able to provide the first provably no-regretemph polynomial time(in the number of evaluations of the acquisition function) algorithm for solving high dimensional BO. To make the optimization efficient and feasible, we introduce a novel deterministic Fourier Features approximation based on numerical integration with detailed analysis for the squared exponential kernel. The error of this approximation decreasesemph exponentially with the number of features, and allows for a precise approximation of both posterior mean and variance. In addition, the kernel matrix inversion improves in its complexity from cubic to essentially linear in the number of data points measured in basic arithmetic operations.","tags":null,"title":"Efficient High Dimensional Bayesian Optimization with Additivity and Quadrature Fourier Features","type":"publication"},{"authors":["Mojmı́r Mutný","Peter Richtárik"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"fd7fb20a1c9a87626941ed28d1014427","permalink":"https://mojmirmutny.github.io/publication/mutny-2018-a/","publishdate":"2021-07-14T08:54:52.984395Z","relpermalink":"/publication/mutny-2018-a/","section":"publication","summary":"We propose a parallel stochastic Newton method (PSN) for minimizing unconstrained smooth convex functions. We analyze the method in the strongly convex case, and give  conditions under which acceleration can be expected when compared to its serial counter- part. We show how PSN can be applied to the large quadratic function minimization in  general, and empirical risk minimization problems. We demonstrate the practical efficiency of the method through numerical experiments and models of simple matrix classes.","tags":null,"title":"Parallel Stochastic Newton Method","type":"publication"},{"authors":["Mojmir Mutný"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"3413189ff029343f3f81b969d0ca8477","permalink":"https://mojmirmutny.github.io/publication/mutny-2016-stochastic/","publishdate":"2021-07-14T08:54:53.436062Z","relpermalink":"/publication/mutny-2016-stochastic/","section":"publication","summary":"A stochastic iterative algorithm approximating second-order information using von Neumann series is discussed. We present convergence guarantees for strongly-convex and smooth functions. Our analysis is much simpler in contrast to a similar algorithm and its analysis, LISSA. The algorithm is primarily suitable for training large scale linear models, where the number of data points is very large. Two novel analyses, one showing space independent linear convergence, and one showing conditional quadratic convergence are discussed. In numerical experiments, the behavior of the error is similar to the second-order algorithm L-BFGS, and improves the performance of LISSA for quadratic objective function.","tags":null,"title":"Stochastic second-order optimization via von Neumann series","type":"publication"}]