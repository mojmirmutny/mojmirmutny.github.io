@misc{borsos2021data,
 abstract = {The increasing availability of massive data sets poses a series of challenges for machine learning. Prominent among these is the need to learn models under hardware or human resource constraints. In such resource-constrained settings, a simple yet powerful approach is to operate on small subsets of the data. Coresets are weighted subsets of the data that provide approximation guarantees for the optimization objective. However, existing coreset constructions are highly model-specific and are limited to simple models such as linear regression, logistic regression, and k-means. In this work, we propose a generic coreset construction framework that formulates the coreset selection as a cardinality-constrained bilevel optimization problem. In contrast to existing approaches, our framework does not require model-specific adaptations and applies to any twice differentiable model, including neural networks. We show the effectiveness of our framework for a wide range of models in various settings, including training non-convex models online and batch active learning.},
 archiveprefix = {arXiv},
 author = {Zalán Borsos and Mojmír Mutný and Marco Tagliasacchi and Andreas Krause},
 eprint = {2109.12534},
 primaryclass = {cs.LG},
 title = {Data Summarization via Bilevel Optimization},
 year = {2021}
}

